Run with python 3!

The main purpose of this progrm is to make observations as
to how the growing number of hidden neurons affects the ability
to converge to low error rates. Also to observe the potentially
different behavior in terms of the error rates on the training
set versus the testing set.

Furthermore, we tabulated our results to try to show how the 
error rates on the training and testing set gradually decreased,
and reached a minimum beyond which no improvement seemed
possible.

This neural network implemented backpropagation of error
for both a predefined number of output and hidden neurons, 
and for a predefined number of attributes. 

Three domains with only numeric attributes from the UCI
repository were be used to train the neural network.

The following learning reate was used: 0.01.

The number of hidden neurons were purposely allowed to vary
in size (roughly 5 - 50).

